#!/usr/bin/env bash
# -----------------------------------------------------------------------------
#  webspider — Spider start URLs/domains/IPs and produce a unique URL list
#
#  WHAT IT DOES (high-level):
#   1) Accepts seeds (full URLs OR scheme-less hostnames/IPs OR a file of seeds)
#   2) Normalizes them (adds scheme, brackets IPv6, adds trailing "/" to dirs)
#   3) Builds a same-site allowlist for wget (so it won’t wander off-site)
#   4) Runs `wget --spider` recursively to discover links (polite by default)
#   5) Parses the wget log, de-duplicates, and filters by mode (video/audio/…)
#   6) Optional sitemaps: sitemap.txt and/or sitemap.xml from the final set
#
#  REQUIREMENTS:
#   - Bash 4+ (arrays and `set -euo pipefail`)
#   - GNU userland: wget, awk, sed, grep, sort, mktemp, paste
#     • Written to be compatible with Debian’s default `mawk`.
#
#  LICENSE & WARRANTY:
#   SPDX-License-Identifier: CC0-1.0
#   NO WARRANTY: This software is provided "AS IS", WITHOUT WARRANTY OF ANY KIND.
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# Usage:
#   webspider [--http|--https]
#             [--video|--audio|--images|--pages|--files|--all]
#             [--ext 'pat|tern']
#             [--delay SECONDS] [--level N|inf] [--status-200]
#             [--no-robots]
#             [--sitemap-txt] [--sitemap-xml]
#             <links.txt | URL...>
#
# Modes (choose one; default is --video):
#   --video    : video files only (mp4|mkv|avi|mov|wmv|flv|webm|m4v|ogv|ts|m2ts)
#   --audio    : audio files only (mp3|mpa|mp2|aac|wav|flac|m4a|ogg|opus|wma|alac|aif|aiff)
#   --images   : image files only (jpg|jpeg|png|gif|webp|bmp|tiff|svg|avif|heic|heif)
#   --pages    : directories (.../) + page-like extensions (html|htm|shtml|xhtml|php|phtml|asp|aspx|jsp|jspx|cfm|cgi|pl|do|action|md|markdown)
#   --files    : all files (exclude directories and .html/.htm pages)
#   --all      : everything (dirs + pages + files)
#
# Options:
#   --ext PAT       : override extension pattern used by --video/--audio/--images/--pages
#   --delay S       : polite crawl delay in seconds (default: 0.5). Always sets --wait=S;
#                     adds --random-wait only when S > 0 (no jitter if S == 0).
#   --level N|inf   : recursion depth for wget (default: inf). N is an integer (0+=).
#   --status-200    : keep URLs that returned HTTP 200 OK (adds -S to wget and parses statuses)
#   --no-robots     : ignore robots.txt (default: respect robots)
#   --sitemap-txt   : write sitemap.txt (newline-separated list from final URLs)
#   --sitemap-xml   : write sitemap.xml (Sitemaps.org XML from final URLs)
#   --http|--https  : default scheme for scheme-less inputs (default: https)
#   -h, --help      : show this help
#
# Notes:
#   - Single-dash forms also work: -video, -audio, -images, -pages, -files, -all, -ext, -delay, -level, -status-200, -no-robots, -sitemap-txt, -sitemap-xml
#   - The spider stays within the seed hosts (and their www. variants). To include more
#     hosts, list them as additional seeds (or via a file).
#   - Use `--` to end options if a path starts with a dash.
#
# Examples:
#   webspider https://www.example.com/
#   webspider --images --delay 1.0 --level 2 example.com/images/
#   webspider --http --status-200 --sitemap-txt --level 1 192.168.1.50:8080 urls.txt
# -----------------------------------------------------------------------------

set -Eeuo pipefail
trap 'echo "[ERROR] $0:$LINENO: $BASH_COMMAND" >&2' ERR

PROG="${0##*/}"

# ---------------------------- Default settings -------------------------------
DEFAULT_SCHEME="https"  # default for scheme-less seeds; can be overridden by --http
MODE="video"            # one of: video|audio|images|pages|files|all
EXT_PATTERN=""          # set automatically per MODE unless user passes --ext
DELAY="0.5"             # polite pacing for wget --wait
LEVEL="inf"             # recursion depth for wget --level
STATUS_200=0            # if 1, keep only URLs with HTTP 200 OK
NO_ROBOTS=0             # if 1, ignore robots.txt; else respect robots
SITEMAP_TXT=0           # write sitemap.txt from final URLs
SITEMAP_XML=0           # write sitemap.xml from final URLs
LOG_FILE="log"          # wget’s log goes here; overwritten each run
OUTPUT_URLS="urls"      # final filtered URL list (deduped)

# ----------------------------- Usage function --------------------------------
usage() {
  cat <<EOF

!!! This script requires the program wget !!!
    sudo apt install wget

Usage:
  $PROG [--http|--https]
        [--video|--audio|--images|--pages|--files|--all]
        [--ext 'pat|tern']
        [--delay SECONDS] [--level N|inf] [--status-200]
        [--no-robots]
        [--sitemap-txt] [--sitemap-xml]
        <links.txt | URL...>

Modes (default: --video):
  --video    : video files only (mp4|mkv|avi|mov|wmv|flv|webm|m4v|ogv|ts|m2ts)
  --audio    : audio files only (mp3|mpa|mp2|aac|wav|flac|m4a|ogg|opus|wma|alac|aif|aiff)
  --images   : image files only (jpg|jpeg|png|gif|webp|bmp|tiff|svg|avif|heic|heif)
  --pages    : directories (.../) + page-like extensions (html|htm|shtml|xhtml|php|phtml|asp|aspx|jsp|jspx|cfm|cgi|pl|do|action|md|markdown)
  --files    : all files (exclude directories and .html/.htm pages)
  --all      : everything (dirs + pages + files)

Options:
  --ext PAT       : override extension pattern used by --video/--audio/--images/--pages
  --delay S       : polite crawl delay in seconds (default: 0.5). Always sets --wait=S; adds
                    --random-wait only when S > 0 (no jitter if S == 0).
  --level N|inf   : recursion depth (wget --level), default: inf
  --status-200    : only keep URLs that returned HTTP 200 OK
  --no-robots     : ignore robots.txt (default is to respect robots)
  --sitemap-txt   : write sitemap.txt from the final URL list
  --sitemap-xml   : write sitemap.xml from the final URL list
  --http|--https  : default scheme for scheme-less inputs
  -h, --help      : show this help

Examples:
  $PROG https://www.example.com/
  $PROG --images --delay 1.0 --level 2 example.com/images/
  $PROG --http --status-200 --sitemap-txt --level 1 192.168.1.50:8080 url-list.txt
     (You can mix literal URLs and a file of URLs.)
EOF
}

# ------------------------------ CLI parsing ----------------------------------
INPUTS=()  # seeds (files or literal seeds)

while [[ $# -gt 0 ]]; do
  arg="$1"
  case "$arg" in
    -h|--help) usage; exit 0 ;;
    -http|--http)   DEFAULT_SCHEME="http"; shift ;;
    -https|--https) DEFAULT_SCHEME="https"; shift ;;
    -video|--video) MODE="video"; shift ;;
    -audio|--audio) MODE="audio"; shift ;;
    -images|--images) MODE="images"; shift ;;
    -pages|--pages) MODE="pages"; shift ;;
    -files|--files) MODE="files"; shift ;;
    -all|--all) MODE="all"; shift ;;
    -ext|--ext)
      [[ $# -ge 2 ]] || { echo "[!] --ext requires a value, e.g. --ext 'jpg|png|gif'"; exit 2; }
      EXT_PATTERN="$2"; shift 2 ;;
    -delay|--delay)
      [[ $# -ge 2 ]] || { echo "[!] --delay requires a value, e.g. --delay 0.9"; exit 2; }
      DELAY="$2"
      [[ "$DELAY" =~ ^[0-9]+([.][0-9]+)?$ ]] || { echo "[!] --delay must be a number, got: $DELAY"; exit 2; }
      shift 2 ;;
    -level|--level)
      [[ $# -ge 2 ]] || { echo "[!] --level requires a value: integer or 'inf'"; exit 2; }
      _lvl_raw="$2"; _lvl_lc="${_lvl_raw,,}"
      if [[ "$_lvl_lc" == "inf" ]]; then
        LEVEL="inf"
      else
        [[ "$_lvl_raw" =~ ^[0-9]+$ ]] || { echo "[!] --level must be an integer or 'inf', got: $_lvl_raw"; exit 2; }
        LEVEL="$_lvl_raw"
      fi
      shift 2 ;;
    -status-200|--status-200) STATUS_200=1; shift ;;
    -no-robots|--no-robots)   NO_ROBOTS=1;  shift ;;
    -sitemap-txt|--sitemap-txt) SITEMAP_TXT=1; shift ;;
    -sitemap-xml|--sitemap-xml) SITEMAP_XML=1; shift ;;
    --) shift; while [[ $# -gt 0 ]]; do INPUTS+=("$1"); shift; done; break ;;
    --*) echo "[!] Unknown option: $1"; usage; exit 2 ;;
    -*)  echo "[!] Unknown option: $1 (did you mean --${1#-} ?)"; usage; exit 2 ;;
    *) INPUTS+=("$1"); shift ;;
  esac
done

if [[ ${#INPUTS[@]} -eq 0 ]]; then
  echo "[!] No inputs provided." >&2
  usage; exit 1
fi

# ------------------------ Mode presets (extensions) --------------------------
if [[ -z "$EXT_PATTERN" ]]; then
  case "$MODE" in
    video)  EXT_PATTERN='mp4|mkv|avi|mov|wmv|flv|webm|m4v|ogv|ts|m2ts' ;;
    audio)  EXT_PATTERN='mp3|mpa|mp2|aac|wav|flac|m4a|ogg|opus|wma|alac|aif|aiff' ;;
    images) EXT_PATTERN='jpg|jpeg|png|gif|webp|bmp|tiff|svg|avif|heic|heif' ;;
    pages)  EXT_PATTERN='html|htm|shtml|xhtml|php|phtml|asp|aspx|jsp|jspx|cfm|cgi|pl|do|action|md|markdown' ;;
    *)      EXT_PATTERN='' ;;
  esac
fi

# ---------------------- Temp files and auto-cleanup --------------------------
TMP_INPUT="$(mktemp)"
TMP_URLS="$(mktemp)"
TMP_FIX="$(mktemp)"
cleanup() { rm -f "${TMP_INPUT}" "${TMP_URLS}" "${TMP_FIX}"; }
trap cleanup EXIT

# -------------------------- Collate the inputs -------------------------------
for src in "${INPUTS[@]}"; do
  if [[ -f "$src" ]]; then
    cat -- "$src" >> "${TMP_INPUT}"
  else
    printf '%s\n' "$src" >> "${TMP_INPUT}"
  fi
done

# ------------------- Prepare log/output, announce settings -------------------
: > "${LOG_FILE}"
: > "${OUTPUT_URLS}"

echo "[*] Normalizing start URLs (scheme: ${DEFAULT_SCHEME}; mode: ${MODE}; delay: ${DELAY}s; level: ${LEVEL}; status-200: ${STATUS_200}; robots: $([[ $NO_ROBOTS -eq 1 ]] && echo off || echo on))"

SCHEME_PREFIX="${DEFAULT_SCHEME}://"

# ------------------- Normalize the seed list into full URLs ------------------
grep -Eo 'https?://[^[:space:]#]+' "${TMP_INPUT}" >> "${TMP_URLS}" || true

{ grep -Ev '^\s*(#|$)' "${TMP_INPUT}" \
  | grep -Ev '^\s*https?://' \
  | sed -E 's/\r$//' \
  | sed -E 's/^\s+|\s+$//g' \
  | awk -v P="$SCHEME_PREFIX" '
      /^\[[0-9A-Fa-f:]+\](:[0-9]+)?(\/.*)?$/ { print P $0; next }
      /^[0-9A-Fa-f:]+(\/.*)?$/ {
          s=$0
          slash=index(s,"/")
          if (slash>0) { host=substr(s,1,slash-1); path=substr(s,slash) }
          else         { host=s; path="" }
          print P "[" host "]" path; next
      }
      NF { print P $0 }
    ' >> "${TMP_URLS}"; } || true

awk '
  {
    u=$0
    if (u ~ /[?#]/) { print u; next }
    if (u ~ /\/$/) { print u; next }
    if (u ~ "^https?://[^/]+$") { print u"/"; next }
    n=split(u,a,"/"); last=a[n]
    if (index(last,".")==0) print u"/"; else print u
  }
' "${TMP_URLS}" > "${TMP_FIX}" && mv "${TMP_FIX}" "${TMP_URLS}"

sort -u -o "${TMP_URLS}" "${TMP_URLS}"

if [[ ! -s "${TMP_URLS}" ]]; then
  echo "No valid URLs found (need full URLs or domain/IP entries)." >&2
  exit 2
fi

# --------------------- Build a same-site allowlist for wget ------------------
HOSTS="$(
  awk -F/ '/^https?:\/\//{print $3}' "${TMP_URLS}" \
  | sed -E 's/^\[([0-9A-Fa-f:]+)\](:[0-9]+)?$/\1/' \
  | sed -E 's/:[0-9]+$//' \
  | awk '{print tolower($0)}' \
  | sort -u
)"
HOSTS="$(echo "${HOSTS}" \
  | awk '/^[a-z0-9-]+\.[a-z0-9-]+$/ {print; print "www."$0; next} {print}' \
  | sort -u)"

USE_DOMAINS=1
if echo "${HOSTS}" | grep -q ':'; then USE_DOMAINS=0; fi

# -------------------------- Assemble the wget call ---------------------------
WGET_CMD=(
  wget
  --spider
  --recursive
  --no-parent
  --level="${LEVEL}"
  --no-directories
  --no-host-directories
  --no-verbose
  --output-file="${LOG_FILE}"
)

# Always set the base wait
WGET_CMD+=( --wait="${DELAY}" )
# Jitter only when delay > 0
case "$DELAY" in
  0|0.*) : ;;
  * )    WGET_CMD+=( --random-wait ) ;;
esac

# Respect robots by default; optionally ignore
if [[ "${NO_ROBOTS}" -eq 1 ]]; then
  WGET_CMD+=( -e robots=off )
fi

# Add status header printing if we need 200 filtering
if [[ "${STATUS_200}" -eq 1 ]]; then
  WGET_CMD+=( -S )
fi

# Domains allowlist (skip when IPv6 literal present)
if [[ ${USE_DOMAINS} -eq 1 ]]; then
  DOMAINS="$(echo "${HOSTS}" | paste -sd, -)"
  WGET_CMD+=( --domains="${DOMAINS}" )
fi

# Add input list
WGET_CMD+=( -i "${TMP_URLS}" )

echo "[*] Starting spider..."
[[ ${USE_DOMAINS} -eq 1 ]] && echo "[*] Domains allowlist: $(echo "${HOSTS}" | paste -sd, -)"
echo "[*] Logging to: ${LOG_FILE}"

# Show the exact wget command (copy-pastable)
{ echo "[*] Wget command:"; printf ' %q' "${WGET_CMD[@]}"; echo; } | tee -a "${LOG_FILE}"

# Run wget; accept 0 (ok) and 8 (broken links found/server errors), fail otherwise
if ! "${WGET_CMD[@]}"; then
  rc=$?
  case "$rc" in
    0) : ;;
    8)
      echo "[!] wget exit 8: server error(s) encountered (e.g., 404/5xx)."
      echo "    This usually just means broken links were found; continuing."
      ;;
    4)
      echo "[!] wget exit 4: network failure. Consider retrying or increasing --delay." >&2
      exit "$rc"
      ;;
    5)
      echo "[!] wget exit 5: SSL verification failure. Check certs or use http/--no-check-certificate." >&2
      exit "$rc"
      ;;
    *)
      echo "[ERROR] wget exited with code $rc; aborting." >&2
      exit "$rc"
      ;;
  esac
fi

echo "[*] Extracting URLs from log..."

if [[ "${STATUS_200}" -eq 1 ]]; then
  # mawk-safe AWK: capture URL token after "URL:" and require same- or next-line 200
  awk '
    /URL:[[:space:]]*https?:\/\// {
      u=$0
      sub(/^.*URL:[[:space:]]*/,"",u)
      sub(/[[:space:]].*$/,"",u)
      last=u
      if ($0 ~ /HTTP\/[0-9.]+[[:space:]]+200([[:space:]]|$)/) { print last; last="" }
      next
    }
    /HTTP\/[0-9.]+[[:space:]]+200([[:space:]]|$)/ && last { print last; last="" }
  ' "${LOG_FILE}" \
  | sed -E 's/[?#].*$//' \
  | sed -E 's#/(index\.html?)$#/#' \
  | sort -u > "${OUTPUT_URLS}"
else
  sed -nE 's/.*URL:[[:space:]]*(https?:\/\/[^ ]+).*/\1/p' "${LOG_FILE}" \
  | sed -E 's/[?#].*$//' \
  | sed -E 's#/(index\.html?)$#/#' \
  | sort -u > "${OUTPUT_URLS}"
fi

# ------------------------- Filter by the selected mode -----------------------
case "${MODE}" in
  video|audio|images)
    mv "${OUTPUT_URLS}" "${OUTPUT_URLS}.all"
    if [[ -n "$EXT_PATTERN" ]] && grep -Eiq "\.(${EXT_PATTERN})$" "${OUTPUT_URLS}.all"; then
      grep -Ei "\.(${EXT_PATTERN})$" "${OUTPUT_URLS}.all" > "${OUTPUT_URLS}"
    else
      : > "${OUTPUT_URLS}"
    fi
    rm -f "${OUTPUT_URLS}.all"
    echo "[*] Mode: ${MODE} only (ext: ${EXT_PATTERN})."
    ;;
  pages)
    mv "${OUTPUT_URLS}" "${OUTPUT_URLS}.all"
    awk -v IGNORECASE=1 -v PATS="${EXT_PATTERN}" '
      /\/$/ { print; next }
      $0 ~ "\\.(" PATS ")$" { print }
    ' "${OUTPUT_URLS}.all" | sort -u > "${OUTPUT_URLS}"
    rm -f "${OUTPUT_URLS}.all"
    echo "[*] Mode: pages only (dirs + ext: ${EXT_PATTERN})."
    ;;
  files)
    mv "${OUTPUT_URLS}" "${OUTPUT_URLS}.all"
    grep -Ev '(/$|\.html?$)' "${OUTPUT_URLS}.all" > "${OUTPUT_URLS}" || true
    rm -f "${OUTPUT_URLS}.all"
    echo "[*] Mode: all files (no dirs/pages)."
    ;;
  all)
    echo "[*] Mode: all URLs (dirs + pages + files)."
    ;;
esac

# ------------------------------ Sitemaps output ------------------------------
if [[ "${SITEMAP_TXT}" -eq 1 ]]; then
  cp "${OUTPUT_URLS}" sitemap.txt
  echo "[*] Wrote sitemap.txt"
fi

if [[ "${SITEMAP_XML}" -eq 1 ]]; then
  esc() { sed -e 's/&/\&amp;/g' -e 's/</\&lt;/g' -e 's/>/\&gt;/g'; }
  {
    echo '<?xml version="1.0" encoding="UTF-8"?>'
    echo '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">'
    while IFS= read -r u; do
      [[ -z "$u" ]] && continue
      echo "  <url><loc>$(printf '%s' "$u" | esc)</loc></url>"
    done < "${OUTPUT_URLS}"
    echo '</urlset>'
  } > sitemap.xml
  echo "[*] Wrote sitemap.xml"
fi

echo "[*] Done. Clean URLs written to: ${OUTPUT_URLS}"
